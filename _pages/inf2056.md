---
layout: archive
title: "INF2056 - Distributed Algorithms"
permalink: /INF2056/
author_profile: true
---

{% include base_path %}

This page is dedicated to the INF2056 - Distributed Algorithms course. 



<details markdown=1>
<summary><b>Simulação de Drone Líder e Seguidor com GrADyS-SIM NextGen</b></summary>

* Neste repositório (https://github.com/sebastianquispearias/test_01_gradys-sim-nextgen), implementei uma simulação simples utilizando o framework GrADyS-SIM NextGen com o objetivo de me familiarizar com suas  funcionalidades. Na simulação, um drone líder (SquareRouteDrone) segue uma rota quadrada e transmite sua posição por meio de plugins de missão e liderança, enquanto um drone seguidor (FollowingDrone) monitora e acompanha esse líder.  É possível visualizar esse comportamento se a linha abaixo for descomentada no `drone_simulation.py`:

```builder.add_handler(VisualizationHandler(VisualizationConfiguration(x_range=(-50, 150), y_range=(-50, 150), z_range=(0, 100), open_browser=True)))```



 
</details>
<br />



<details markdown=1>
<summary><b>Mapeamento e Detecção de Pessoas com Swarm de Drones</b></summary>

A ideia central do nosso projeto é desenvolver um sistema distribuído para missões de busca e salvamento (SAR) que permita construir um mapa global com todos os pontos de interesse (POIs) detectados (neste caso, pessoas), utilizando dados capturados a bordo dos drones. Cada drone, equipado com uma câmera (como a Raspberry Pi HQ Camera) e com informações de GPS armazenadas nos metadados EXIF, processa localmente as imagens para identificar automaticamente os POIs, utilizando modelos de detecção de objetos (por exemplo, YOLO ou alternativas). Essa detecção gera um output com as coordenadas geográficas de cada pessoa encontrada, que são posteriormente compartilhadas entre todos os drones do enxame.

Inicialmente, o swarm de drones irá viajando em uma formação em "V", realizando uma rota determinada, e posteriormente (com todos os POIs já mapeados), o sistema aplicará uma função objetivo para minimizar a distância e o tempo de voo, permitindo que cada drone seja designado para visitar um ou mais pontos de interesse de maneira otimizada. Essa abordagem colaborativa evita soluções subótimas que ocorreriam se os drones operassem de forma isolada, e também otimiza os recursos durante a missão.

Um dos pontos que geram mais preocupação são as seguintes situações: se dois drones detectam a mesma pessoa, mas, devido a pequenas diferenças de tempo ou erros de medição, cada um gera coordenadas ligeiramente diferentes para esse POI. Nesta situação, em lugar de ter 1 POI, teríamos 2 POIs. Por outro lado, se um drone detecta uma vítima e outro não a detecta, o sistema pode perder a chance de registrar esse POI como válido [não estou seguro sobre o que deveria acontecer nessa situação]. Por esse motivo, assumiremos inicialmente que os POIs são fixos e, posteriormente, exploraremos soluções para não assumir necessariamente isso (talvez utilizando filtro de Kalman ou técnicas como DBSCAN).

Finalmente, estes são alguns dos papers que serão estudados e que, se for pertinente, poderão ser usados em nossa solução:

[1]Real Time Embedded Image Processing System for Points of Interest Detection for Autonomous Unmanned Aerial Vehicles – Freitas et al.

[2]Robot Swarm Navigation and Victim Detection Using Rendezvous Consensus in Search and Rescue Operations – Cardona et al.

[3]A Survey of Deep Learning Techniques for Vehicle Detection from UAV Images – Srivastava et al.

[4]Unmanned Aerial Vehicles (UAVs): Practical Aspects, Applications, Open Challenges, Security Issues, and Future Trends – Mohsan et al.

[5]Vehicle Detection from UAV Imagery with Deep Learning: A Review – Bouguettaya et al.


 
</details>
<br />



<details markdown=1>
<summary><b>camera_extension_problem</b></summary>

A ideia deste cenário é simular um enxame de drones explorando uma área quadrada L×L e recolher os IDs de cada PoI de forma coordenada, procurando maximizar o número de identificações únicas dentro do tempo de missão definido. Utilizamos a nova extensão de câmera implementada no simulador (totalmente configurável em campo de visão e alcance). Código e instruções completos em:
https://github.com/sebastianquispearias/camera_extension_problem

Mission Overview:
O objetivo é que um Exploration Quadcopter (E-QC) patrulhe sistematicamente a área, detectando PoIs não mapeados e classificando-os por níveis de urgência — fazendo uso da extensão de câmera para parametrizar alcance e visão. Em seguida, coordenamos essa descoberta com vários Visiting Quadcopters (V-QCs), que receberão as coordenadas e se mobilizarão para extrair o ID de cada PoI.

Exploration Quadcopter (E-QC):
O E-QC percorre continuamente waypoints predefinidos e, a cada segundo, chama take_picture() para capturar a cena. Ele aplica a heurística urgência ÷ distância para filtrar PoIs, armazena cada coordenada de PoI com seu timestamp de detecção e envia mensagens ASSIGN aos V-QCs contendo coordenada, urgência e hora da detecção.

Visiting Quadcopters (V-QCs):
Os V-QCs deambularão aleatoriamente até preencherem next2visit. Ao chegar a cada coordenada, comparam posições com uma tolerância mínima, coletam o ID do PoI e retornam ao explorador via DELIVER. Se o buffer ficar vazio, retomam automaticamente o roaming.

Além dessas funcionalidades, implementei registro de timestamps em cada detecção (para análise de latências) e contagem de visitas redundantes (para medir quantas vezes um mesmo PoI é processado e ajustar a eficiência da coordenação).

 
</details>
<br />
